# ServiceNow Incident Auto-Assignment ML Model

## ğŸ¯ Project Overview

This is a comprehensive end-to-end Machine Learning solution for automatically assigning ServiceNow incidents to the appropriate team/personnel based on incident details. The system uses NLP and ML techniques to analyze incident descriptions and predict the best assignment group.

## ğŸ“‹ Features

### Data Preprocessing
- Drop irrelevant columns
- Handle null values and duplicates
- Text cleaning (lowercase conversion, special characters removal)
- Remove common words, stop words, URLs, email IDs, phone numbers
- Remove file paths and normalize spacing

### Feature Engineering
- Tokenization
- Count Vectorizer
- TF-IDF Vectorizer
- Word2Vec embeddings
- Label Encoding
- Named Entity Recognition (NER)
- POS Tagging

### Machine Learning Models
**Traditional ML:**
- K-Nearest Neighbors (KNN)
- Support Vector Machine (SVM)
- Gradient Boosting
- Naive Bayes
- Decision Tree
- Random Forest
- Logistic Regression
- Stochastic Gradient Descent (SGD)

**Deep Learning:**
- Deep Neural Network (DNN)
- Recurrent Neural Network (RNN)
- Long Short-Term Memory (LSTM)
- Level 1 / Level 2 Modeling (Ensemble)

### Model Evaluation
- Accuracy
- F1 Score (Precision & Recall)
- Cohen Kappa Score
- Loss metrics
- Hyperparameter tuning
- Ensemble methods
- Model comparison charts

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- pip or conda

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd Ml_Model_SNOW
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Download NLTK data:**
```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('averaged_perceptron_tagger'); nltk.download('maxent_ne_chunker'); nltk.download('words')"
```

## ğŸ“ Project Structure

```
Ml_Model_SNOW/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Place your raw datasets here
â”‚   â”œâ”€â”€ processed/                    # Processed datasets
â”‚   â””â”€â”€ sample/                       # Sample data for testing
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_EDA.ipynb                 # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_Data_Preprocessing.ipynb  # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ 03_Feature_Engineering.ipynb # Feature engineering experiments
â”‚   â”œâ”€â”€ 04_Model_Training.ipynb      # Model training and evaluation
â”‚   â””â”€â”€ 05_Model_Comparison.ipynb    # Compare all models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py          # Text cleaning functions
â”‚   â”‚   â””â”€â”€ data_loader.py           # Data loading utilities
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py     # Feature engineering
â”‚   â”‚   â””â”€â”€ vectorizers.py           # Vectorization methods
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ traditional_ml.py        # Traditional ML models
â”‚   â”‚   â”œâ”€â”€ deep_learning.py         # Deep learning models
â”‚   â”‚   â””â”€â”€ ensemble.py              # Ensemble methods
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py               # Evaluation metrics
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                # Configuration
â”‚       â””â”€â”€ helpers.py               # Helper functions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ saved_models/                # Trained models
â”‚   â””â”€â”€ checkpoints/                 # Model checkpoints
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/                       # Visualization plots
â”‚   â”œâ”€â”€ reports/                     # Evaluation reports
â”‚   â””â”€â”€ logs/                        # Training logs
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                     # Training script
â”‚   â”œâ”€â”€ predict.py                   # Prediction script
â”‚   â”œâ”€â”€ evaluate.py                  # Evaluation script
â”‚   â””â”€â”€ deploy.py                    # Deployment script
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_preprocessing.py        # Unit tests
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ setup.py                         # Package setup
â”œâ”€â”€ config.yaml                      # Configuration file
â””â”€â”€ README.md                        # This file
```

## ğŸ’» Usage

### 1. Place Your Data
Put your datasets in the `data/raw/` folder:
- `AnyConv.com__incident (1).csv` - Historical incidents data
- `AMS_ACC_Incident_KPIs (1).xlsx` - Incident KPIs

### 2. Run Exploratory Data Analysis
```bash
jupyter notebook notebooks/01_EDA.ipynb
```

### 3. Train Models
```bash
# Train all models
python scripts/train.py --config config.yaml

# Train specific model
python scripts/train.py --model random_forest

# Train with custom parameters
python scripts/train.py --model lstm --epochs 50 --batch-size 32
```

### 4. Evaluate Models
```bash
python scripts/evaluate.py --model-path models/saved_models/best_model.pkl
```

### 5. Make Predictions
```bash
# Predict from file
python scripts/predict.py --input data/new_incidents.csv --output predictions.csv

# Single prediction
python scripts/predict.py --text "Unable to access email application"
```

## ğŸ“Š Model Performance

Results will be saved in `results/reports/model_comparison.csv` with metrics for all models.

## ğŸ”§ Configuration

Edit `config.yaml` to customize:
- Data paths
- Model hyperparameters
- Feature engineering options
- Training parameters

## ğŸ³ Docker Support (Optional)

```bash
docker build -t ml-snow-incident .
docker run -p 5000:5000 ml-snow-incident
```

## ğŸ“ API Documentation

After training, you can deploy the model as an API:

```bash
python scripts/deploy.py
```

API will be available at `http://localhost:5000`

**Endpoints:**
- `POST /predict` - Make predictions
- `GET /health` - Health check
- `GET /metrics` - Model metrics

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

MIT License

## ğŸ‘¥ Authors

- Your Name

## ğŸ™ Acknowledgments

- ServiceNow for the use case
- Open source ML community

## ğŸ“ Support

For issues and questions, please open an issue on GitHub.

---

**Last Updated:** December 2025
