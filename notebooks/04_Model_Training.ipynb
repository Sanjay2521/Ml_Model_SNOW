{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Comparison\n",
    "## ServiceNow Incident Auto-Assignment\n",
    "\n",
    "This notebook trains multiple ML models and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from src.preprocessing import TextCleaner, DataLoader, remove_duplicates, handle_null_values\n",
    "from src.features import FeatureExtractor, TextVectorizer\n",
    "from src.models import TraditionalMLModels\n",
    "from src.evaluation import ModelEvaluator\n",
    "from src.utils import load_config, format_time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config.yaml')\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = DataLoader('../config.yaml')\n",
    "csv_path = Path('../data/raw/') / config['data']['incident_csv']\n",
    "\n",
    "if csv_path.exists():\n",
    "    df = data_loader.load_csv(str(csv_path))\n",
    "else:\n",
    "    print(\"Using sample data...\")\n",
    "    from src.preprocessing import load_sample_data\n",
    "    df = load_sample_data()\n",
    "\n",
    "print(f\"Initial shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df = handle_null_values(df, strategy='fill')\n",
    "df = remove_duplicates(df)\n",
    "\n",
    "# Clean text\n",
    "text_cleaner = TextCleaner(config.get('preprocessing', {}))\n",
    "text_columns = [col for col in ['short_description', 'description'] if col in df.columns]\n",
    "\n",
    "if text_columns:\n",
    "    df = text_cleaner.clean_dataframe(df, text_columns)\n",
    "\n",
    "print(f\"After preprocessing: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "target_col = config.get('data', {}).get('target_column', 'assignment_group')\n",
    "\n",
    "train_df, val_df, test_df = data_loader.split_data(\n",
    "    df, target_col,\n",
    "    train_size=0.8, val_size=0.1, test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TextVectorizer(config)\n",
    "vectorizer.fit_tfidf_vectorizer(train_df['cleaned_text'].tolist())\n",
    "\n",
    "X_train = vectorizer.transform_tfidf_vectorizer(train_df['cleaned_text'].tolist())\n",
    "X_test = vectorizer.transform_tfidf_vectorizer(test_df['cleaned_text'].tolist())\n",
    "\n",
    "y_train = train_df[target_col].values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and evaluator\n",
    "ml_models = TraditionalMLModels(config)\n",
    "evaluator = ModelEvaluator(config)\n",
    "\n",
    "models_to_train = ['logistic_regression', 'random_forest', 'naive_bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "training_times = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    model = ml_models.train_model(model_name, X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    training_times[model_name] = training_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = ml_models.predict(model_name, X_test)\n",
    "    y_pred_proba = ml_models.predict_proba(model_name, X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluator.calculate_all_metrics(y_test, y_pred, y_pred_proba, model_name)\n",
    "    evaluator.print_metrics(model_name)\n",
    "    \n",
    "    print(f\"Training time: {format_time(training_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison_df = evaluator.compare_models()\n",
    "comparison_df['training_time'] = comparison_df.index.map(training_times)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "evaluator.plot_model_comparison(\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    figsize=(16, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_name = comparison_df['f1_score'].idxmax()\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"F1 Score: {comparison_df.loc[best_model_name, 'f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "y_pred_best = ml_models.predict(best_model_name, X_test)\n",
    "\n",
    "evaluator.plot_confusion_matrix(\n",
    "    y_test, y_pred_best,\n",
    "    title=f'Confusion Matrix - {best_model_name}',\n",
    "    figsize=(10, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "evaluator.print_classification_report(y_test, y_pred_best, model_name=best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "ml_models.save_model(best_model_name, f'../models/saved_models/best_model.pkl')\n",
    "print(f\"Best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook trained and compared multiple ML models for incident auto-assignment. The best performing model has been saved and can be used for predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
